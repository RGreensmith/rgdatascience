{
    "title":{"content": "Classification Algorithms", "className": "title--blue"},
    
    "content":[
        {"type": "paragraph","content":"Classification algorithms are supervised machine learning techniques (rather than unsupervised, semi-supervised and reinforcement learning), which means that the main aim is to find the best model which predicts the y for new x data 3. Furthermore, a unique attribute of supervised learning algorithms is that they are given a set of instances with a y feature, also known as dependent variable or target, with the input data 3, 7. Classification algorithms are used for tasks where the y variable is discrete 3. When y has two classes, a binary classification approach is taken, whereas, in the instance where the y has more than two classes, a multi-class classification approach must be taken. There are two main approaches for multi-class problems: one vs all and one vs one. When there are n classes of y, the one vs all approach only uses n binary classification models, in contrast, the one vs one approach uses:",
            "className": "inline"},
        {"type": "image","content":"classifierExplanationImages/CE0.png","className":"inline"},
        {"type": "paragraph","content":"classification models. The focus of this article is to explain my understanding of classification techniques for binary class problems.",
            "className": "inline"},
        {"type": "paragraph","content": "Binary classification algorithms aim to predict whether y is 0 or 1, based on probability, by finding the optimal decision boundary within the feature space and using a mapping function 7. The feature space is an n dimensional space of input features, where n is determined by the number of independent variables, which are commonly referred to as features in machine learning literature. Classification algorithms can be split by the type of learner they are, a lazy learner or an eager learner 7. Lazy learners wait until test data is presented, then use the most relevant training data to train on, whereas eager learners make a model using all training data before test data is provided, and so the instance space is taken up with one hypothesis. In contrast to eager learners, a lazy learner takes less time to train but more time when making predictions. K-nearest neighbour and case-based reasoning are examples of lazy learners, and decision trees, naïve bayes and artificial neural networks are examples of eager learners."},
        {"type": "paragraph","content": "A classification problem is linear if the classes can be separated linearly, if not, then the problem is non-linear 7. Some classifiers perform better with linear data and others with non-linear. Examples of linear classifiers are logistic regression 17, support vector machine 19 and naïve bayes 9, 17, and examples of non-linear classifiers are k-nearest neighbours 9, 11, kernel support vector machine 20, decision tree 11 and random forest 11."},
        {"type": "paragraph", "content": "Examples of linear and non-linear data with a linear and non-linear classifier fitted are demonstrated in figure 1. The left-hand column is the input data (synthetic for concept visualisation), the top row of graphs are linear data, and the bottom two rows are non-linear data. The centre column shows naïve bayes classifier (a linear classifier), and the right-hand column shows K-nearest neighbours’ classifier (non-linear classifier). The accuracy scores (in the bottom corner of each naïve bayes and k-nearest neighbours plot) show that naïve bayes performed better with the linearly separable data (top centre, score 0.95), compared to k-nearest neighbours (top right, score 0.93), and compared to the non-linear data (centre, score 0.90; bottom centre, score 0.70). On the other hand, k-nearest neighbours performed better with the non-linear data (centre right, score 0.97; bottom right, score 0.93)."},
        {"type": "image","content": "classifierExplanationImages/CE1.png", "className":"center"},
        {"type": "paragraph","content":"Figure 1. Examples of linear and non-linear data fitted with a linear and non-linear classifier. Synthetic data generated from the sklearn dataset library."},
        {"type": "paragraph", "content":"Classifiers can also be frequency based or geometry based 26. Examples of frequency-based classifiers are Bayes algorithms. Examples of geometry-based classifiers are K-nearest neighbour algorithms."},
        {"type": "subheading2","content": "Linear Classifiers"},
        {"type": "subheading","content": "Logistic Regression"},
        {"type": "paragraph","content":"To calculate the logistic regression curve, take the linear regression formula and apply a sigmoid function to y (from linear regression formula):"},
        {"type":"image","content":"classifierExplanationImages/CE2.png", "className":"center"},
        {"type": "paragraph","content": "(Linear regression formula)"},
        {"type": "image","content": "classifierExplanationImages/CE3.png",
            "className": "center"},
        {"type": "paragraph","content": "(Applying sigmoid function to y)"},
        {"type": "paragraph","content": "Resulting in the logistic regression formula:"},
        {"type": "image","content": "classifierExplanationImages/CE4.png",
            "className": "center"},
        {"type":"paragraph","content":"The algorithm finds the line of best fit from which is used to predict the probability of the y (dependent variable) being 1, rather than 0, where the y axis is probability from 0 to 1."},
        {
            "type": "list",
            "items": [
                "If p^ < 0.5 , then y^ = 0",
                "If p^ ≥ 0.5 , then y^ = 1"
            ]
        },
        {"type":"paragraph","content":"Where p^ is probability, and y^ is the prediction of y (the dependent variable), however the 50% boundary of p^ is arbitrary, and can be set to the user’s preference, based on knowledge of the task. 0 and 1 represent the two classes of the dependent variable, so 0 could equal “apple” and 1 could equal “pear” if the task is to predict whether a fruit is an apple or pear."},
        {"type": "subheading","content": "Naïve Bayes"},
        {"type": "paragraph", "content": "This classifier is named naïve bayes as it is constructed around the bayes theorem, and since it requires an assumption of independence, yet there is not always complete independence between the independent variables in real-world data, it is naïve to assume."},
        {"type": "paragraph", "content": "The following is a made-up classification problem (using artificial data) to explain bayes theorem, inspired by research published by Jones et al. (2020). Jones et al. (2020) found evidence supporting niche partitioning amongst females from a colony on Bird Island (South Georgia), highlighted by difference in carbon isotope signature of the fur seals whiskers, which was used as a proxy for foraging habitat. Jones et al. (2020) reported that, of the 20 females sampled, 70% were found to have carbon isotope signatures, indicating most of the foraging activity being south of the polar front, whereas 30% of the females spent most of their foraging activity north of the polar front. The group mostly foraging south of the polar front were found have lower nitrogen isotope values (indicating feeding at a lower trophic level), and were smaller in body size, than those mostly foraging north of the polar front (Jones et al., 2020)."},
        {"type": "paragraph", "content": "In this artificial case-study, to make it more interesting, I loosely based the generation of the dataset on the findings of Jones et al. (2020). The two independent variables are the average nitrogen isotope values of the fur seals vibrissae (whiskers), and estimated age (approximated by whisker growth rate), and the dependent variable is the area where a female fur seal spends more time foraging; either north or south of the polar front."},
        {"type": "paragraph","content": "In this example, the question is: what is the probability of a new data point of a seal foraging north of the polar front, where only nitrogen isotope value and age is known?"},
        {"type": "paragraph","content": "The naïve bayes classifier uses the following equation:"},
        {"type": "image","content": "classifierExplanationImages/CE8.png",
            "className": "center"},
        {"type":"paragraph","content": "Where P is the probability, A is a class of the dependent variable and B represents the independent variables. Therefore, the question can be rewritten as:"},
        {"type": "image","content": "classifierExplanationImages/CE9.png",
            "className": "center"},
        {"type": "paragraph","content":"Which can be calculated using the bayes theorem equation, which is rewritten with the example below:"},
        {"type": "image","content": "classifierExplanationImages/CE10.png",
            "className": "center"},
        {"type": "paragraph","content":""},
        {"type": "image","content": "classifierExplanationImages/CE11.png",
            "className": "center"},
        {"type": "paragraph","content": "In this example, there are 40 seals in total:"},
        {
            "type": "list",
            "items": [
                "12 seals forage north of the polar front",
                "28 seals forage south of the polar front"
            ]
        },
        {
            "type": "paragraph",
            "content": "1. Firstly, calculate the prior probability:"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE12.png",
            "className": "center"
        },
        {
            "type": "paragraph",
            "content": ""
        },
        {
            "type": "paragraph",
            "content": "2. Then calculate the marginal likelihood:"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE13.png",
            "className": "center"
        },
        {
            "type": "paragraph",
            "content": "To find the number of similar observations, construct a radius around the new data point (the size can be altered) within the feature space. Any data point within the radius is deemed similar in features to the new data point, so P(X) is the probability of any random data point being similar in features to the new data point (being in the radius)"
        },
        {
            "type": "paragraph",
            "content": "In this example, within the radius around the new data point, there are 2 seals foraging north of the polar front and 3 seals foraging south of the polar front. Therefore:"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE15.png",
            "className": "center"
        },
        {
            "type": "paragraph",
            "content": "where number of similar observations is the number of observations within the radius."
        },
        {
            "type": "paragraph",
            "content": ""
        },
        {
            "type": "paragraph",
            "content": "3. Thirdly, calculate the likelihood:"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE16.png",
            "className": "center"
        },
        {
            "type": "paragraph",
            "content": "which is the probability of any random data point (that forages north) being similar in features to the new data point (being in the radius)"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE17.png",
            "className": "center"
        },
        {
            "type": "paragraph",
            "content": ""
        },
        {
            "type": "paragraph",
            "content": "4. Finally, the posterior probability:"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE18.png",
            "className": "center"
        },
        {
            "type": "paragraph",
            "content": "can then be calculated."
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE19.png",
            "className": "center"
        },
        {
            "type": "paragraph",
            "content": "The probability of the new seal foraging south of the polar front can be calculated in the same way:"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE20.png",
            "className": "center"
        },
        {
            "type": "subheading",
            "content": "Support Vector Machine"
        },
        {
            "type": "paragraph",
            "content": "To classify data, the classes are divided by a hyperplane called the maximum margin hyperplane. This hyperplane is positioned at the maximum distance between a small subset of datapoints from the different classes which are close together in the feature space. These datapoints are called the support vectors, as they support the position of the maximum margin hyperplane, which would not maintain its position without each support vector."
        },
        {
            "type": "paragraph",
            "content": "There are also two planes that each intersects the support vectors of the two classes and is parallel to the maximum margin hyperplane. These are called the negative and positive hyperplanes. The datapoints that fall outside of the space between the negative or positive hyperplane and the maximum margin hyperplane, is classed as either a negative or positive vector, depending on whether it falls outside of the negative hyperplane or the positive hyperplane."
        },
        {
            "type": "subheading2",
            "content": "Non-Linear Classifiers"
        },
        {
            "type": "paragraph",
            "content": ""
        },
        {
            "type": "subheading",
            "content": "Kernel Support Vector Machine"
        },
        {
            "type": "paragraph",
            "content": "There are many types of kernel functions which can be used, the common ones being radial basis function (also known as gaussian kernel), sigmoid kernel and polynomial kernel functions. The radial basis function is the kernel explained and used in this article. The following equation is the radial basis function kernel:"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE21.png",
            "className": "center"
        },
        {
            "type": "paragraph",
            "content": "K stands for kernel, which is a function applied to two vectors:",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE22.png", "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "(a datapoint) and ",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE23.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "which is the landmark vector (of which there can be multiple). E is an exponent to the minus power of the squared distance (represented by double vertical lines) between the x vector and the landmark vector, divided by 2 times sigma squared. Sigma is a fixed parameter that is decided earlier in the process and is further explained below.",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "The previously used example of female Antarctic fur seals with most foraging activity either north or south of the polar front as the dependent variable, and nitrogen isotope value and age as independent variables will be used, but where the artificial data is non-linear. Figure RBF is a visual representation for understanding the basic concept of how RBF kernel SVM works, with two features (x and y axes), which does not map to higher dimensions; the third-dimension aspect (z axis) of the figure visually represents the",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE24.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "values assigned to the points.",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "In figure RBF, the yellow point represents the location of a landmark, the green points represent seals mostly foraging south of the polar front, and the red points represent seals mostly foraging north of the polar front. When the radial basis function is applied to all of the points in the grid, the shape of the",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE24.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "values around the landmark follows a normal distribution, which is visually demonstrated by the three-dimensional surface mesh grid (figure RBF), where low ",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE24.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "values are blue and high ",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE24.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "values are red. ",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "When a datapoint is far away from the landmark, such as the green point with the yellow arrow reaching between it and the landmark (representing the distance measurement), the ",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE24.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "value for that point will be close to zero, as the distance between the two vectors",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE25.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "will be a greater value, so as a negative exponent,",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE26.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "it will make the base extremely close to zero, demonstrated by the green point being at the bottom of the z axis in figure rbf. However, when a datapoint is close to the landmark, such as the red point with the yellow arrow reaching between it and the landmark (representing the distance measurement), the ",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE27.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "value for that point will be closer to 1, as the distance value being smaller makes the negative exponent a larger value, which is demonstrated by the red point being ‘projected’ up the z axis, to a position higher up on the surface mesh peak. The ",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE27.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "is always between zero and 1, as a kernel is a . The red datapoints that are further from the landmark in figure rbf are not projected as high on the z axis as the datapoints closer to the landmark, which have ",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE27.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "values closer to 1. After the datapoints have had the radial basis function applied to them, they can now be linearly separated. The white dashed circle around the base of the structure in figure rbf is the decision boundary for the classes, the circumference of which is controlled by the value of sigma. A lower sigma decreases the circumference, which would position the decision boundary higher on the vertical structure, which would mean in this example, that the",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE27.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "value of a new datapoint must be higher to be assigned as a seal which mostly forages north of the polar front.",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE28.png",
            "className": "center"
        },
        {"type": "paragraph", "content": "Figure RBF. Green: Seals mostly foraging South of the polar front, Red: Seals mostly foraging North of the polar front, yellow point: landmark, yellow arrows distances between landmark and two randomly selected points."},
        {
            "type": "subheading",
            "content": "K Nearest Neighbours"
        },
        {
            "type": "paragraph",
            "content": "K nearest neighbours classifies new data based on the most occurring class of k number of datapoints, closest to the new datapoint. Firstly, the value of k must be selected, which affects how sensitive the classifier is to noise, so needs to be tuned to the optimal value for a parsimonious model. A common approach to choosing a k value to start with, is",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE29.png",
            "className": "inline"
        },
        {
            "type": "paragraph",
            "content": "where n is the number of datapoints in the training set, also choosing an odd number of k. Then, out of the k closest datapoints (the nearest neighbours), the modal class is assigned to the new datapoint. A variety of metrics can be used to calculate the distance between the new data and the k nearest neighbours, although Euclidean distance is often the default. Euclidean distance calculates the distance between points in a straight line, as the hypotenuse of a right-angled triangle. For example:",
            "className": "inline"
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE30.png",
            "className": "center"
        },
        {
            "type": "subheading",
            "content": "Decision Tree"
        },
        {
            "type": "paragraph",
            "content": "The decision tree classification algorithm creates splits in the data between the different classes, making one split across one axis at a time. The points within the split boundaries are called leaves, and the algorithm aims to find the optimal position to split the data, where the leaves would have the maximum number of points. The final leaves are called the terminal leaves. When a new datapoint is being classified, it is evaluated by the decision tree, where each node is a question, with a yes or no answer, and when the terminal leaf is reached, the new datapoint is assigned the class that occupies that leaf. The questions are based on the split, for example, split 1 intersects X1 at 0.4, so the first question is whether the data point has X1 value less than 0.4. If no, then the datapoint is assigned ‘south’ (blue point), if yes, then another node is met. The question of this node is whether the datapoint has an X2 value less than 0.3 (which is where split 2 intersects the X2 axis); if yes, then the datapoint is assigned ‘south’ (blue point), if no, then it is assigned ‘north’ (red point)."
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE31.png",
            "className": "center"
        },
        {
            "type": "paragraph",
            "content": ""
        },
        {
            "type": "image",
            "content": "classifierExplanationImages/CE32.png",
            "className": "center"
        },
        {
            "type": "subheading",
            "content": "Random Forest"
        },
        {
            "type": "paragraph",
            "content": "Random forest is an ensemble classifier model, which means that it makes use of multiple models to predict new data, assigning the most frequently predicted class of each model to the new data. In the case of random forest, the classifier models used are decision tree classifiers. Firstly, a random sample of k number of datapoints from the training set are selected. A decision tree is then built for each of the k number of random sample sets. When predicting the class of new data, each of the decision trees provide a prediction of the class of the new data. The class that the random forest classifier assigns to the new data is the most frequently predicted class from all the decision trees."
        },


        {}
        
    ]
}

